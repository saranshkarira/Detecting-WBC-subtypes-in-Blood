{
  "cells": [
    {
      "metadata": {
        "_uuid": "951f0b76cfc12953949a1d56f33ebf4ff7e9fdf2"
      },
      "cell_type": "markdown",
      "source": "# Blood Cell Classification\nSubtypes : Eosinophil, Lymphocyte, Monocyte and Neutrophil"
    },
    {
      "metadata": {
        "_uuid": "592bfe3af218041f2e5c1cd7a6af92e46eb1b781"
      },
      "cell_type": "markdown",
      "source": "## Importing Modules"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport os\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd.variable import Variable\nimport pandas as pd\nimport cv2\n\n%matplotlib inline\n%pylab inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "47e29516b25b5a15e48fc958207f774f9d21b3de"
      },
      "cell_type": "markdown",
      "source": "## Organize"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "19230bc5b6b68e10a32ea70efe21524247df61c5"
      },
      "cell_type": "code",
      "source": "def pplus(*things):\n    sentence = \"\"\n    for i in things:\n        if not isinstance(i, str):\n            i  =  str(i)\n        sentence += \" \" +i\n    print('------------------------------------')\n    print(sentence)\n    print('------------------------------------')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5618a077e0f644316fe0c908d740b6b0146b96c7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "DIR = \"../input/dataset2-master/dataset2-master/images/\"\nTEST = \"../input/dataset2-master/dataset2-master/images/TEST/\"\nTRAIN = \"../input/dataset2-master/dataset2-master/images/TRAIN/\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "537d423c95eae62299f98f70bcb68d0a6f516edb"
      },
      "cell_type": "code",
      "source": "batch_size = 128\nlr = 0.0002",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "030c846529d75537620b52b6ea1264118a5bb53f"
      },
      "cell_type": "markdown",
      "source": "## Inspecting"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "pplus(\"Top dir:\", os.listdir(\"../input\"))\npplus(\"dataset2 dir:\", os.listdir(DIR))\npplus(\"TEST_SIMPLE:\", os.listdir(DIR+\"TEST_SIMPLE\"))\npplus('samples in one TS dir:', len(os.listdir(DIR+\"TEST_SIMPLE/MONOCYTE\")))\npplus('samples in another TS dir:', len(os.listdir(DIR+\"TEST_SIMPLE/EOSINOPHIL\")))\nprint('Looks like Class imbalance, Plot a histogram to get proper estimate')\npplus(len(os.listdir(TEST+\"MONOCYTE\")))\npplus(len(os.listdir(TEST+\"EOSINOPHIL\")))\nprint('Looks like augmented set doesnot have any imbalance')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d2a80f834e67db13a03e18fc8ff0a86648ce3ea",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "mappings = dict(zip(['NEUTROPHIL', 'EOSINOPHIL', 'MONOCYTE','LYMPHOCYTE'],list(range(0,4))))\nprint(mappings)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "14234b7703e9f960810b862da6d40ed6175217a1",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "labels = pd.read_csv('../input/dataset2-master/dataset2-master/labels.csv')\npplus(labels.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9da094aeac10b0f87b1356151d486aefa226fc73",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "pplus(labels.head())\npplus(labels.tail())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d7e71dfa5b0a59b5d6223f416806b7d131f4281c"
      },
      "cell_type": "markdown",
      "source": "## Plotting a Sample of each"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e62248b834d6649e5819b6dad2b3a6b86c52e61f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(16,16))\n\ndef plot_image(location, subplot):\n    title = location.split('/')[0]\n    location = TRAIN+location\n    plt.subplot(subplot)\n    plt.title(title)\n    plt.axis('off')\n    plt.imshow(cv2.imread(location))\n    return\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7772ccfc615d19f63365f566e67c7b45b7c00c07",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "loc = []\nloc.append('EOSINOPHIL/_0_207.jpeg')\nloc.append('LYMPHOCYTE/_0_204.jpeg')\nloc.append('MONOCYTE/_0_180.jpeg')\nloc.append('NEUTROPHIL/_0_292.jpeg')\n\nsubplot = 221\nfor i in loc:\n    plot_image(i, subplot)\n    subplot += 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "4ef1ff85d700690e7a2e78b556dc9929ed56397a"
      },
      "cell_type": "code",
      "source": "from tqdm import tqdm\ndef get_data(folder):\n    im = []\n    labels = []\n    for subtype in os.listdir(folder):\n        if not subtype.startswith('.'):\n            label = mappings[subtype]\n        for img_name in tqdm(os.listdir(folder+subtype)):\n            im.append(cv2.resize(cv2.imread(folder+subtype+'/'+img_name), (64,64)))\n            labels.append(label)\n    return np.asarray(im), np.asarray(labels)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58b988d8c0c5e6d0855b554feb7c1f19111a01bf",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_im, train_labels = get_data(TRAIN)\ntest_im, test_labels = get_data(TEST)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8eb872c51380c5dad577a6088e756d8b46b74a82"
      },
      "cell_type": "markdown",
      "source": "## Create Custom Dataset Class"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "18de423c57e38f10d4e3c613a798b01f18ca6d54",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "class BloodDataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        if self.transform:\n            return self.transform(self.images[idx]), self.labels[idx]\n        return self.images[idx], self.labels[idx]\n        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bad2853b1a2614974b23fbc5aaa093b41742487f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\ntfms = transforms.Compose([transforms.ToTensor()])\ntrain = BloodDataset(train_im, train_labels, transform=tfms)\n\n\nmean = tuple((train_im.mean(axis=(0,1,2))/255).round(4))\nstd = tuple((train_im.std(axis=(0,1,2))/255).round(4))\n\ntfms = transforms.Compose([transforms.ToTensor(),\n                           transforms.Normalize(mean, std)])\n\ntrain = BloodDataset(train_im, train_labels, transform=tfms)\n\npplus('Mean Values:',mean)\npplus('Std Dev',std)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e8481a4ed2a633a99203ac3ff145fdf2cc294075",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "tfms = transforms.Compose([transforms.ToTensor()])\ntest = BloodDataset(test_im, test_labels, transform=tfms)\n\nmean = tuple((test_im.std(axis=(0,1,2))/255).round(4))\nstd = tuple((test_im.std(axis=(0,1,2))/255).round(4))\n\ntfms = transforms.Compose([transforms.ToTensor(),\n                            transforms.Normalize(mean, std)])\ntest = BloodDataset(test_im, test_labels, transform=tfms)\n\npplus('Mean Values:', mean)\npplus('Std Dev', std)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9047d9d467da8be508ba34597e9a4cc11058ccc",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Only available in pytorch 0.4.1\n# valid, training = torch.utils.data.random_split(train, len(train_labels)//10, len(train_labels)-len(train_labels)//10 )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d2878d2c510e5d692469f227e16f6c8acb60bc4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train.__getitem__(3)[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "313f3e58a5362b4be95f9d55191120aa19bfdb77"
      },
      "cell_type": "code",
      "source": "trainloader = DataLoader(train, batch_size=batch_size, shuffle=True)\ntestloader = DataLoader(test, batch_size=64, shuffle=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a448a28537eb3026d34e3e1a7c076a77e255dc75"
      },
      "cell_type": "markdown",
      "source": "## Perfect! All looks good, now let's define our network\n- Let's call it depth seperable wide resnet."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "979cc01056b77882c0f3a8e86ed7c2b3e263f008",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\n\nclass Network(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer0 = self.conv(3,32)\n        self.block1 = Block(32)\n        self.block2 = Block(64)\n        self.block3 = Block(128)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(256, 4)\n\n    def conv(self, in_, out_): \n        return nn.Sequential(\n            nn.Conv2d(in_, out_, 3, 1, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_))\n        \n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.avgpool(x)\n        x = x.view(-1, 256)\n        x = self.fc(x)\n        return x\n# ----------------------------------------------------------------------------------------#\nclass Block(nn.Module):\n    def __init__(self, in_):\n        super().__init__()\n        self.layer1 = self.conv_ds(in_, in_*2)\n        self.layer2 = self.conv_ds(in_*2, in_*2)\n        self.layer2_1 = self.conv_bn(in_*2, in_*2)\n        \n        self.layer1_1x1 = self.one_by_one(in_, in_*2)\n\n    def one_by_one(self, in_, out_):\n        return nn.Sequential(\n            nn.Conv2d(in_, out_, 1, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_)\n        )\n    def conv_bn(self, in_, out_):\n        return nn.Sequential(\n            nn.Conv2d(in_, out_, 3, 2, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_))\n    \n    def conv_ds(self, in_, out_): \n        mid = round(in_ * 3/2)\n        return nn.Sequential(\n            nn.Conv2d(in_, mid, 1, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(mid),\n            nn.Conv2d(mid, mid, 3, 1, 1, groups=mid, bias=False),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(mid),\n            nn.Conv2d(mid, out_, 1, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_))\n    def forward(self, x):\n        x = self.layer1(x) + self.layer1_1x1(x)\n        x = self.layer2(x) + x\n        x = self.layer2_1(x)\n        return x",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d15265e291437b2baa6afed0a0fb212cb9dc01f",
        "scrolled": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(Network())\nif not torch.cuda.is_available(): pplus('Turn ON your GPU')\nnet = Network()\nnet = net.cuda()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d0fc4cff9eb061db99fb1fc02b141dcd2b74c543"
      },
      "cell_type": "markdown",
      "source": "## Create a Training Regime"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9b5cb895ed4ca632f8a95c770a11da329295d8b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.85, weight_decay=1e-4)\noptimizer = optim.Adam(net.parameters(), lr=lr, betas=(0.5, 0.999))\ncriterion = nn.CrossEntropyLoss()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "175b11a6f98df233e6825ceca2d595fb07e70d47"
      },
      "cell_type": "code",
      "source": "def train(epoch):\n    net.train()\n    train_loss = 0\n    counter = 0\n    accuracy = 0\n    for batch_idx, (data, targets) in enumerate(trainloader):\n        data, targets = Variable(data).cuda(), Variable(targets).cuda()\n        optimizer.zero_grad()\n        outputs = net(data)\n        loss = criterion(outputs, targets)\n        train_loss += loss.data.cpu().numpy().round(5)\n        counter += 1\n        loss.backward()\n        optimizer.step()\n        \n#         ticks = np.zeros(targets.size(0))\n#         ticks[outputs.data.cpu().numpy().argmax(1)[1] == targets.data.cpu().numpy()] = 1\n#         accuracy = ticks.sum()/len(target)\n\n        _, predicted = torch.max(outputs.data,1)\n        accuracy = (predicted == targets).sum()\n        \n        if batch_idx%10 == 0:\n            pplus('Train Epoch:', epoch, batch_idx*len(data),'/',len(trainloader.dataset),\n                  '|\\n Train Loss:',loss.data.cpu().numpy().round(5),\n                  '|\\n Accuracy:', (accuracy*100/labels.data.size(0).round(5)\n                 )\n    return train_loss/counter",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "8efc9d4243356e367359fffb4035879e29bc8c1a"
      },
      "cell_type": "code",
      "source": "train_stack = []",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ed1c4dea57f59e3d9c7f634c44a8d9cdb8a4e7e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\nfor i in range(200):\n    train_stack.append(train(i))\n    plt.plot(train_stack)\n    plt.xlabel('Epochs --->')\n    plt.ylabel('Loss --->')\n    plt.title('Train Loss')\n    plt.show()\n#     test_stack.append(test(i))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa69a24011a4c76c9c8d3d761a9f731fb87cdef4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.plot(train_stack)\nplt.xlabel('Epochs --->')\nplt.ylabel('Loss --->')\nplt.title('Train Loss')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e8940316f080c0dbb724da164e3714e7256b81d3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "maza = {'train_loss': train_stack}\ndf = pd.DataFrame(maza)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d59047bd00f74c10e27702fdeca1c504fdeb1535",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df.to_csv('log.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "283205b40acddb5538f4bda6109d09935722bb7d"
      },
      "cell_type": "code",
      "source": "print(train_stack)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "66f221b2422caf40ab062096183413c6ce12e8c2"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}